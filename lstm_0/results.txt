#1. NO preprocessing
Train...
Train on 1611 samples, validate on 378 samples
Epoch 1/5
1611/1611 [==============================] - 33s - loss: 0.6916 - acc: 0.5369 - val_loss: 0.6920 - val_acc: 0.5132
Epoch 2/5
1611/1611 [==============================] - 31s - loss: 0.6252 - acc: 0.6741 - val_loss: 0.7322 - val_acc: 0.5423
Epoch 3/5
1611/1611 [==============================] - 33s - loss: 0.3523 - acc: 0.8678 - val_loss: 0.8534 - val_acc: 0.5370
Epoch 4/5
1611/1611 [==============================] - 33s - loss: 0.1022 - acc: 0.9721 - val_loss: 1.2540 - val_acc: 0.5661
Epoch 5/5
1611/1611 [==============================] - 32s - loss: 0.0274 - acc: 0.9969 - val_loss: 1.5331 - val_acc: 0.5529
378/378 [==============================] - 1s
Test score: 1.53309273089
Test accuracy: 0.55291005291
Generating test predictions...
prediction accuracy:  0.55291005291
end

[('the', 26697), ('to', 21248), ('of', 18659), ('in', 18568), ('and', 10295), ('for', 7691), ('on', 6381), ('is', 5574), ('that', 4172), ('by', 4123), ('with', 3776), ('has', 3726), ('as', 3531), ('from', 3503), ('it', 3064), ('are', 2945), ('at', 2826), ('have', 2769), ('be', 2566), ('us', 2499), ('an', 2416), ('after', 2356), ('was', 2158), ('over', 2059), ('israel', 1948), ('not', 1922), ('new', 1871), ('will', 1862), ('says', 1841), ('world', 1829), ('they', 1765), ('china', 1729), ('government', 1724), ('who', 1712), ('its', 1708), ('their', 1674), ('police', 1666), ('people', 1555), ('his', 1510), ('been', 1462), ('war', 1411), ('more', 1392), ('up', 1381), ('against', 1339), ('out', 1338), ('russia', 1325), ('he', 1265), ('year', 1240), ('000', 1219), ('than', 1198)]
there is "000" above
[('the', 26697), ('to', 21248), ('of', 18659), ('in', 18569), ('and', 10295), ('for', 7691), ('on', 6381), ('is', 5574), ('that', 4172), ('by', 4123), ('with', 3776), ('has', 3726), ('as', 3531), ('from', 3503), ('it', 3064), ('are', 2945), ('at', 2826), ('have', 2769), ('be', 2566), ('us', 2502), ('an', 2416), ('after', 2356), ('was', 2158), ('over', 2059), ('israel', 1948), ('not', 1922), ('new', 1872), ('will', 1862), ('says', 1841), ('world', 1829), ('they', 1765), ('china', 1729), ('government', 1724), ('who', 1712), ('its', 1708), ('their', 1674), ('police', 1666), ('people', 1555), ('his', 1510), ('been', 1462), ('war', 1411), ('more', 1392), ('up', 1381), ('against', 1339), ('out', 1338), ('russia', 1325), ('he', 1265), ('year', 1241), ('than', 1198), ('years', 1196)]
remove 0-9 above

did not remove 0-9
Train...
Train on 1611 samples, validate on 378 samples
Epoch 1/3
1611/1611 [==============================] - 39s - loss: 0.6912 - acc: 0.5295 - val_loss: 0.6937 - val_acc: 0.5079
Epoch 2/3
1611/1611 [==============================] - 34s - loss: 0.6485 - acc: 0.6487 - val_loss: 0.6870 - val_acc: 0.5556
Epoch 3/3
1611/1611 [==============================] - 34s - loss: 0.3958 - acc: 0.8839 - val_loss: 0.8217 - val_acc: 0.5608
378/378 [==============================] - 1s
Test score: 0.821731014226
Test accuracy: 0.560846562423
Generating test predictions...
prediction accuracy:  0.560846560847

#2. remove stopwords from NLTK stopwords corpus
Train...
Train on 1611 samples, validate on 378 samples
Epoch 1/5
1611/1611 [==============================] - 30s - loss: 0.6912 - acc: 0.5313 - val_loss: 0.6936 - val_acc: 0.5079
Epoch 2/5
1611/1611 [==============================] - 28s - loss: 0.6259 - acc: 0.6952 - val_loss: 0.7118 - val_acc: 0.5291
Epoch 3/5
1611/1611 [==============================] - 28s - loss: 0.2780 - acc: 0.9205 - val_loss: 0.9848 - val_acc: 0.5185
Epoch 4/5
1611/1611 [==============================] - 28s - loss: 0.0654 - acc: 0.9808 - val_loss: 1.3789 - val_acc: 0.5026
Epoch 5/5
1611/1611 [==============================] - 28s - loss: 0.0119 - acc: 0.9975 - val_loss: 1.6279 - val_acc: 0.5265
378/378 [==============================] - 2s
Test score: 1.62791126935
Test accuracy: 0.526455026613
Generating test predictions...
prediction accuracy:  0.526455026455
end


